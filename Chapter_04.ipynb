{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ef500a",
   "metadata": {},
   "source": [
    "# Chapter 4 â€“ Reflection and Introspection in Agents\n",
    "---\n",
    "\n",
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6158ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai ipywidgets crewai pysqlite3-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478113ec",
   "metadata": {},
   "source": [
    "# 1. Meta Reasoning - example\n",
    "---\n",
    "\n",
    "Let's take a look at a simple meta-reasoning approach without AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac6d01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee2139",
   "metadata": {},
   "source": [
    "## Simulated travel agent with meta-reasoning capabilities\n",
    "\n",
    "- recommend_destination: The agent recommends a destination based on user preferences (budget, luxury, adventure) and internal weightings.\n",
    "\n",
    "- get_user_feedback: The agent receives feedback on the recommendation (positive or negative).\n",
    "\n",
    "- meta_reasoning: The agent adjusts its reasoning by updating the weights based on feedback, improving future recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa6edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated travel agent with meta-reasoning capabilities\n",
    "class ReflectiveTravelAgent:\n",
    "    def __init__(self):\n",
    "        # Initialize preference weights that determine how user preferences influence recommendations\n",
    "        self.preferences_weights = {\n",
    "            \"budget\": 0.5,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.3,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.2  # Weight for adventure-related preferences\n",
    "        }\n",
    "        self.user_feedback = []  # List to store user feedback for meta-reasoning\n",
    "\n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"\n",
    "        Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "        Args:\n",
    "            user_preferences (dict): User's preferences with keys like 'budget', 'luxury', 'adventure'\n",
    "\n",
    "        Returns:\n",
    "            str: Recommended destination\n",
    "        \"\"\"\n",
    "        # Calculate scores for each destination based on weighted user preferences\n",
    "        score = {\n",
    "            \"Paris\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] + \n",
    "                      self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"Bangkok\": (self.preferences_weights[\"budget\"] * user_preferences[\"budget\"] +\n",
    "                        self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"New York\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "                         self.preferences_weights[\"budget\"] * user_preferences[\"budget\"])\n",
    "        }\n",
    "        # Select the destination with the highest calculated score\n",
    "        recommendation = max(score, key=score.get)\n",
    "        return recommendation\n",
    "\n",
    "    def get_user_feedback(self, actual_experience):\n",
    "        \"\"\"\n",
    "        Simulate receiving user feedback and trigger meta-reasoning to adjust recommendations.\n",
    "\n",
    "        Args:\n",
    "            actual_experience (str): The destination the user experienced\n",
    "        \"\"\"\n",
    "        # Simulate user feedback: 1 for positive, -1 for negative\n",
    "        feedback = random.choice([1, -1])\n",
    "        print(f\"Feedback for {actual_experience}: {'Positive' if feedback == 1 else 'Negative'}\")\n",
    "        \n",
    "        # Store the feedback for later analysis\n",
    "        self.user_feedback.append((actual_experience, feedback))\n",
    "        \n",
    "        # Trigger meta-reasoning to adjust the agent's reasoning process based on feedback\n",
    "        self.meta_reasoning()\n",
    "\n",
    "    def meta_reasoning(self):\n",
    "        \"\"\"\n",
    "        Analyze collected feedback and adjust preference weights to improve future recommendations.\n",
    "        This simulates the agent reflecting on its reasoning process and making adjustments.\n",
    "        \"\"\"\n",
    "        for destination, feedback in self.user_feedback:\n",
    "            if feedback == -1:  # Negative feedback indicates dissatisfaction\n",
    "                # Reduce the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 0.9  # Decrease luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "            elif feedback == 1:  # Positive feedback indicates satisfaction\n",
    "                # Increase the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 1.1  # Increase luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "\n",
    "        # Normalize weights to ensure they sum up to 1 for consistency\n",
    "        total_weight = sum(self.preferences_weights.values())\n",
    "        for key in self.preferences_weights:\n",
    "            self.preferences_weights[key] /= total_weight\n",
    "\n",
    "        # Display updated weights after meta-reasoning adjustments\n",
    "        print(f\"Updated weights: {self.preferences_weights}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ad2f0",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "- User Preferences: Defines the user's preferences for budget, luxury, and adventure.\n",
    "\n",
    "- First Recommendation: The agent recommends a destination based on the initial weights and user preferences.\n",
    "\n",
    "- User Feedback Simulation: Simulates the user providing feedback on the recommended destination.\n",
    "\n",
    "- Second Recommendation: After adjusting the weights based on feedback, the agent makes a new recommendation that reflects the updated reasoning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29fc98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended destination: Bangkok\n",
      "Feedback for Bangkok: Positive\n",
      "Updated weights: {'budget': 0.5238095238095238, 'luxury': 0.2857142857142857, 'adventure': 0.19047619047619047}\n",
      "\n",
      "Updated recommendation: Bangkok\n"
     ]
    }
   ],
   "source": [
    "# Simulate agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = ReflectiveTravelAgent()\n",
    "\n",
    "    # User's initial preferences\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.8,      # High preference for budget-friendly options\n",
    "        \"luxury\": 0.2,      # Low preference for luxury\n",
    "        \"adventure\": 0.5    # Moderate preference for adventure activities\n",
    "    }\n",
    "\n",
    "    # First recommendation based on initial preferences and weights\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Recommended destination: {recommended}\")\n",
    "\n",
    "    # Simulate user experience and provide feedback\n",
    "    agent.get_user_feedback(recommended)\n",
    "\n",
    "    # Second recommendation after adjusting weights based on feedback\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Updated recommendation: {recommended}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cb609",
   "metadata": {},
   "source": [
    "## Meta-reasoning with AI\n",
    "---\n",
    "\n",
    "Now let's bring in AI to perform meta-reasoning with agents. In this case we will use CrewAI framework to create our meta-reasoning Agents with OpenAI LLMs. We will also emulate a user feedback using AI just for demonstration purposes. First, let's make sure we initialize our OpenAI API key and then let's define the \"Crew\" (with CrewAI) and the Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "api_key = getpass.getpass(prompt=\"Enter OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69487942",
   "metadata": {},
   "source": [
    "We will define three tools that our agents will use-\n",
    "\n",
    "1. `recommend_destination`: This tool will use a set of base weights that prioritizes budget, luxury, and adventure equally and then uses user's preference weights to recommend a destination. Paris will emphasize luxury, NYC emphasizes luxury and adventure, whereas Bangkok emphasizes budget.\n",
    "2. `update_weights_on_feedback`: This tool will update the internal base weights based on the user's feedback on the recommended destination. A positive feedback will tell the model that it's recommendation is correct and it needs to update it's internal base weights based and increase it by a given (arbitrary adjustment factor), or reduce the weights using the adjustment factor if the feedback is dissatisfied.\n",
    "3. `feedback_emulator`: This tool will emulate a user prividing \"satisfied\" or \"dissatisfied\" feedback to the AI agent's destination recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a18088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "\n",
    "# Tool 1\n",
    "@tool(\"Recommend travel destination based on preferences.\")\n",
    "def recommend_destination(user_preferences: dict) -> str:\n",
    "    \"\"\"\n",
    "    Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "    Args:\n",
    "        user_preferences (dict): User's preferences with keys - 'budget', 'luxury', 'adventure'\n",
    "                                default user_preference weights 'budget' = 0.8, 'luxury' = 0.2, 'adventure' = 0.5\n",
    "                                user_preferences = {\n",
    "                                                \"budget\": 0.8,\n",
    "                                                \"luxury\": 0.4,\n",
    "                                                \"adventure\": 0.3\n",
    "                                            }\n",
    "    Returns:\n",
    "        str: Recommended destination\n",
    "    \"\"\"\n",
    "    internal_default_weights = {\n",
    "            \"budget\": 0.33,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.33,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.33  # Weight for adventure-related preferences\n",
    "        }\n",
    "   # Calculate weighted scores for each destination\n",
    "    score = {\n",
    "        \"Paris\": (\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] +      # Paris emphasizes luxury\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"] +\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"]\n",
    "        ),\n",
    "        \"Bangkok\": (\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"] * 2 +  # Bangkok emphasizes budget\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"]\n",
    "        ),\n",
    "        \"New York\": (\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] * 1.5 +  # NYC emphasizes luxury and adventure\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"] * 1.5 +\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Select the destination with the highest calculated score\n",
    "    recommendation = max(score, key=score.get)\n",
    "    return recommendation\n",
    "\n",
    "# Tool 2\n",
    "@tool(\"Reasoning tool to adjust preference weights based on user feedback.\")\n",
    "def update_weights_on_feedback(destination: str, feedback: int, adjustment_factor: float) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze collected feedback and adjust internal preference weights based on user feedback for better future recommendations.\n",
    "\n",
    "    Args:        \n",
    "        destination (str): The destination recommended ('New York', 'Bangkok' or 'Paris')\n",
    "        feedback (int): Feedback score; 1 = Satisfied, -1 = dissatisfied\n",
    "        adjustment_factor (int): The adjustment factor between 0 and 1 that will be used to adjust the internal weights.\n",
    "                                 Value will be used as (1 - adjustment_factor) for dissatisfied feedback and (1 + adjustment_factor)\n",
    "                                 for satisfied feedback.\n",
    "    Returns:\n",
    "        dict: Adjusted internal weights\n",
    "    \"\"\"\n",
    "    internal_default_weights = {\n",
    "        \"budget\": 0.33,    # Weight for budget-related preferences\n",
    "        \"luxury\": 0.33,    # Weight for luxury-related preferences\n",
    "        \"adventure\": 0.33  # Weight for adventure-related preferences\n",
    "    }\n",
    "\n",
    "    # Define primary and secondary characteristics for each destination\n",
    "    destination_characteristics = {\n",
    "        \"Paris\": {\n",
    "            \"primary\": \"luxury\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        },\n",
    "        \"Bangkok\": {\n",
    "            \"primary\": \"budget\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        },\n",
    "        \"New York\": {\n",
    "            \"primary\": \"luxury\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Get the characteristics for the given destination\n",
    "    dest_chars = destination_characteristics.get(destination, {})\n",
    "    primary_feature = dest_chars.get(\"primary\")\n",
    "    secondary_feature = dest_chars.get(\"secondary\")\n",
    "\n",
    "    # adjustment_factor = 0.2  # How much to adjust weights by\n",
    "\n",
    "    if feedback == -1:  # Negative feedback\n",
    "        # Decrease weights for the destination's characteristics\n",
    "        if primary_feature:\n",
    "            internal_default_weights[primary_feature] *= (1 - adjustment_factor)\n",
    "        if secondary_feature:\n",
    "            internal_default_weights[secondary_feature] *= (1 - adjustment_factor/2)\n",
    "            \n",
    "    elif feedback == 1:  # Positive feedback\n",
    "        # Increase weights for the destination's characteristics\n",
    "        if primary_feature:\n",
    "            internal_default_weights[primary_feature] *= (1 + adjustment_factor)\n",
    "        if secondary_feature:\n",
    "            internal_default_weights[secondary_feature] *= (1 + adjustment_factor/2)\n",
    "\n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    total_weight = sum(internal_default_weights.values())\n",
    "    for key in internal_default_weights:\n",
    "        internal_default_weights[key] = round(internal_default_weights[key] / total_weight, 2)\n",
    "\n",
    "    # Ensure weights sum to exactly 1.0 after rounding\n",
    "    adjustment = 1.0 - sum(internal_default_weights.values())\n",
    "    if adjustment != 0:\n",
    "        # Add any rounding difference to the largest weight\n",
    "        max_key = max(internal_default_weights, key=internal_default_weights.get)\n",
    "        internal_default_weights[max_key] = round(internal_default_weights[max_key] + adjustment, 2)\n",
    "\n",
    "    return internal_default_weights\n",
    "\n",
    "# Tool 3\n",
    "@tool(\"User feedback emulator tool\")\n",
    "def feedback_emulator(destination: str) -> int:\n",
    "    \"\"\"\n",
    "    Given a destination recommendation (such as 'New York' or 'Bangkok') this tool will emulate to provide\n",
    "    a user feedback as 1 (satisfied) or -1 (dissatisfied)\n",
    "    \"\"\"\n",
    "    import random\n",
    "    feedback = random.choice([-1, 1])\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe973f1c",
   "metadata": {},
   "source": [
    "Once, the tools are defined, we will declare three CrewAI Agents each of which will use one of the tools above. The `meta_agent` is basically the agent that will perform meta-reasoning using the emulated user feedback and the previously recommended destination to update the internal weights using an `adjustment_factor`. \n",
    "\n",
    "Note that here, the model assigns an adjustment factor dynamically to adjust the internal system weights (which is `{\"budget\": 0.33, \"luxury\": 0.33, \"adventure\": 0.33}` in the beginning), i.e. we are not hard coding the adjustment factor. Although, the nature of user feedback in this example is limited to \"satisfied\" or \"dissatisfied\" (1 or -1), feedback can be of various forms and may contain more details, in which case your AI Agent may adjust different values to the adjustment_factor. More contextual feedback with details will help the model perform better meta-reasoning on it's previous responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06e6b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel destination recommender\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUse the recommend_destination tool with these preferences: {'budget': 0.1, 'luxury': 0.42, 'adventure': 0.47}\n",
      "Return only the destination name as a simple string (Paris, Bangkok, or New York).\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel destination recommender\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to analyze the user's preferences regarding budget, luxury, and adventure to provide an appropriate recommendation.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mRecommend travel destination based on preferences.\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"user_preferences\\\": {\\\"budget\\\": 0.1, \\\"luxury\\\": 0.42, \\\"adventure\\\": 0.47}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "New York\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel destination recommender\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "New York\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSimulated feedback provider\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUse the feedback_emulator tool with the destination from the previous task.\n",
      "Instructions:\n",
      "1. Get the destination string from the previous task\n",
      "2. Pass it directly to the feedback_emulator tool\n",
      "3. Return the feedback value (1 or -1)\n",
      "\n",
      "IMPORTANT: Pass the destination as a plain string, not a dictionary.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSimulated feedback provider\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI should proceed to emulate feedback for the destination \"New York\".\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mUser feedback emulator tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"destination\\\": \\\"New York\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "1\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSimulated feedback provider\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "1\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPreference weight adjuster\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUse the update_weights_on_feedback tool with:\n",
      "1. destination: Get from first task's output (context[0])\n",
      "2. feedback: Get from second task's output (context[1])\n",
      "3. adjustment_factor: a number betweek 0 and 1 that will be used to adjust internal weights based on feedback\n",
      "\n",
      "Ensure all inputs are in their correct types (string for destination, integer for feedback).\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPreference weight adjuster\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI should gather the necessary input for the update_weights_on_feedback tool. From the context, I have the destination as \"New York\" and the feedback as 1 (satisfied). Next, I need to determine a suitable adjustment factor between 0 and 1. I will select a common midpoint value of 0.5 for this task.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mReasoning tool to adjust preference weights based on user feedback.\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"destination\\\": \\\"New York\\\", \\\"feedback\\\": 1, \\\"adjustment_factor\\\": 0.5}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'budget': 0.27, 'luxury': 0.4, 'adventure': 0.33}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPreference weight adjuster\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{'budget': 0.27, 'luxury': 0.4, 'adventure': 0.33}\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "Final Results: {'budget': 0.27, 'luxury': 0.4, 'adventure': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception while exporting Span batch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 294, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 294, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/sdk/trace/export/__init__.py\", line 360, in _export_batch\n",
      "    self.span_exporter.export(self.spans_list[:idx])  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 189, in export\n",
      "    return self._export_serialized_spans(serialized_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 159, in _export_serialized_spans\n",
      "    resp = self._export(serialized_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 133, in _export\n",
      "    return self._session.post(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/adapters.py\", line 682, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Exception while exporting Span batch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 294, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 294, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/sdk/trace/export/__init__.py\", line 360, in _export_batch\n",
      "    self.span_exporter.export(self.spans_list[:idx])  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 189, in export\n",
      "    return self._export_serialized_spans(serialized_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 159, in _export_serialized_spans\n",
      "    resp = self._export(serialized_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 133, in _export\n",
      "    return self._session.post(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/adapters.py\", line 682, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Exception while exporting Span batch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 294, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/urllib3/connection.py\", line 507, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 325, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 294, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/sdk/trace/export/__init__.py\", line 360, in _export_batch\n",
      "    self.span_exporter.export(self.spans_list[:idx])  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 189, in export\n",
      "    return self._export_serialized_spans(serialized_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 159, in _export_serialized_spans\n",
      "    resp = self._export(serialized_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 133, in _export\n",
      "    return self._session.post(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anjanavb/Documents/Packt_Book/packt-book/packt/lib/python3.11/site-packages/requests/adapters.py\", line 682, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from typing import Dict, Union\n",
    "import random\n",
    "\n",
    "# Utility functions\n",
    "def process_recommendation_output(output: str) -> str:\n",
    "    \"\"\"Extract the clean destination string from the agent's output.\"\"\"\n",
    "    # Handle various ways the agent might format the destination\n",
    "    for city in [\"Paris\", \"Bangkok\", \"New York\"]:\n",
    "        if city.lower() in output.lower():\n",
    "            return city\n",
    "    return output.strip()\n",
    "\n",
    "def process_feedback_output(output: Union[Dict, str]) -> int:\n",
    "    \"\"\"Extract the feedback value from the agent's output.\"\"\"\n",
    "    if isinstance(output, dict):\n",
    "        return output.get('feedback', 0)\n",
    "    try:\n",
    "        # Try to parse as integer if it's a string\n",
    "        return int(output)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "def generate_random_preferences():\n",
    "    # Generate 3 random numbers and normalize them\n",
    "    values = [random.random() for _ in range(3)]\n",
    "    total = sum(values)\n",
    "    \n",
    "    return {\n",
    "        \"budget\": round(values[0]/total, 2),\n",
    "        \"luxury\": round(values[1]/total, 2),\n",
    "        \"adventure\": round(values[2]/total, 2)\n",
    "    }\n",
    "\n",
    "# Initial shared state for weights, preferences, and results\n",
    "state = {\n",
    "    \"weights\": {\"budget\": 0.33, \"luxury\": 0.33, \"adventure\": 0.33},\n",
    "    \"preferences\": generate_random_preferences()\n",
    "}\n",
    "\n",
    "# Agents\n",
    "preference_agent = Agent(\n",
    "    name=\"Preference Agent\",\n",
    "    role=\"Travel destination recommender\",\n",
    "    goal=\"Provide the best travel destination based on user preferences and weights.\",\n",
    "    backstory=\"An AI travel expert adept at understanding user preferences.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[recommend_destination]\n",
    ")\n",
    "\n",
    "feedback_agent = Agent(\n",
    "    name=\"Feedback Agent\",\n",
    "    role=\"Simulated feedback provider\",\n",
    "    goal=\"Provide simulated feedback for the recommended travel destination.\",\n",
    "    backstory=\"An AI that mimics user satisfaction or dissatisfaction for travel recommendations.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[feedback_emulator]\n",
    ")\n",
    "\n",
    "meta_agent = Agent(\n",
    "    name=\"Meta-Reasoning Agent\",\n",
    "    role=\"Preference weight adjuster\",\n",
    "    goal=\"Reflect on feedback and adjust the preference weights to improve future recommendations.\",\n",
    "    backstory=\"An AI optimizer that learns from user experiences to fine-tune recommendation preferences.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[update_weights_on_feedback]\n",
    ")\n",
    "\n",
    "\n",
    "# Tasks with data passing\n",
    "generate_recommendation = Task(\n",
    "    name=\"Generate Recommendation\",\n",
    "    agent=preference_agent,\n",
    "    description=(\n",
    "        f\"Use the recommend_destination tool with these preferences: {state['preferences']}\\n\"\n",
    "        \"Return only the destination name as a simple string (Paris, Bangkok, or New York).\"\n",
    "    ),\n",
    "    expected_output=\"A destination name as a string\",\n",
    "    output_handler=process_recommendation_output\n",
    ")\n",
    "\n",
    "simulate_feedback = Task(\n",
    "    name=\"Simulate User Feedback\",\n",
    "    agent=feedback_agent,\n",
    "    description=(\n",
    "        \"Use the feedback_emulator tool with the destination from the previous task.\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Get the destination string from the previous task\\n\"\n",
    "        \"2. Pass it directly to the feedback_emulator tool\\n\"\n",
    "        \"3. Return the feedback value (1 or -1)\\n\\n\"\n",
    "        \"IMPORTANT: Pass the destination as a plain string, not a dictionary.\"\n",
    "    ),\n",
    "    expected_output=\"An integer feedback value: 1 or -1\",\n",
    "    context=[generate_recommendation],\n",
    "    output_handler=process_feedback_output\n",
    ")\n",
    "\n",
    "adjust_weights = Task(\n",
    "    name=\"Adjust Weights Based on Feedback\",\n",
    "    agent=meta_agent,\n",
    "    description=(\n",
    "        \"Use the update_weights_on_feedback tool with:\\n\"\n",
    "        \"1. destination: Get from first task's output (context[0])\\n\"\n",
    "        \"2. feedback: Get from second task's output (context[1])\\n\"\n",
    "        \"3. adjustment_factor: a number betweek 0 and 1 that will be used to adjust internal weights based on feedback\\n\\n\"\n",
    "        \"Ensure all inputs are in their correct types (string for destination, integer for feedback).\"\n",
    "    ),\n",
    "    expected_output=\"Updated weights as a dictionary\",\n",
    "    context=[generate_recommendation, simulate_feedback]\n",
    ")\n",
    "\n",
    "# Crew Definition\n",
    "crew = Crew(\n",
    "    agents=[preference_agent, feedback_agent, meta_agent],\n",
    "    tasks=[generate_recommendation, simulate_feedback, adjust_weights],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "result = crew.kickoff()\n",
    "print(\"\\nFinal Results:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104c35a",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Self Explanation - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dd7e0",
   "metadata": {},
   "source": [
    "\n",
    "In this section we will see how to implement transparency, learning and refinement, user engagement & collaboration.\n",
    "\n",
    "1. **Self-Explanation transparency**: For each recommendation, the agent generates a detailed self-explanation. This explanation outlines the factors that led to the recommendation, such as proximity to popular attractions, budget-friendly options, or the presence of adventure activities. The purpose is to provide transparency into how the decision was made, helping the user understand the reasoning process.\n",
    "\n",
    "2. **Learning and refinement**: The agent doesn't stop after making the recommendation. It actively reflects on user feedback (whether positive or negative). If the feedback is negative, it introspects on its decision-making process and adjusts the importance (weights) it assigns to user preferences for future recommendations. For instance, if a user dislikes a budget-friendly recommendation, the agent might reduce the emphasis it places on budget-related preferences.\n",
    "\n",
    "3. **User Engagement**: The class also simulates a dialogue with the user. After giving the recommendation and the self-explanation, it collects feedback from the user, allowing for a more collaborative interaction. This feedback is then used to refine future recommendations, making the agent more adaptive and personalized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44f3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "api_key = getpass.getpass(prompt=\"Enter OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f241f2",
   "metadata": {},
   "source": [
    "### 2.1 Transparency: Verbalizing Reasoning in Decisions\n",
    "\n",
    "Lets use OpenAI SDK to see how a model can perform reasoning in the decisions it makes. Here, the agent generates explanations for its reasoning when recommending a travel itinerary. It uses GPT-4o-mini to generate self-explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062e1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Self-Explanation:\n",
      "Sure! When making a hotel recommendation for a user traveling to Paris, there are several key factors to consider based on the given preferences and criteria. Here's how I would arrive at a recommendation:\n",
      "\n",
      "1. **Proximity to Popular Attractions**: Paris is renowned for its iconic landmarks, such as the Eiffel Tower, the Louvre, and Notre-Dame Cathedral. Staying near these attractions can significantly enhance a visitor's experience by reducing travel time and allowing for greater flexibility to explore at their leisure. Thus, I would prioritize hotels that are within walking distance or a short metro ride to these major sites.\n",
      "\n",
      "2. **High Ratings from Similar Travelers**: User ratings provide insight into the quality of the hotel, service, cleanliness, and overall satisfaction from previous guests. To ensure a positive experience, I would look for hotels that have a high average rating (usually 4 stars and above) from travelers who have similar interests or preferences. Reviews can also reveal specific details about the hotel that might be crucial for the traveler, such as quiet rooms, friendly staff, or good local dining options.\n",
      "\n",
      "3. **Competitive Pricing Within Budget**: Since the user has a budget of $200, I would focus on hotels that fall within this price range. Paris can be expensive, so itâ€™s important to find accommodations that offer good value for money while still meeting the userâ€™s needs and expectations. I would filter my options to ensure they fit this financial criterion while still meeting the other factors.\n",
      "\n",
      "4. **Summary of User Preferences**: As the user highlighted a preference for both \"proximity to attractions\" and \"user ratings,\" my recommendation would balance these two aspects. I would choose a hotel that is not only well-rated but also situated conveniently for accessing various key sights and experiences in Paris.\n",
      "\n",
      "Given these considerations, I might recommend a hotel like \"Hotel La Comtesse,\" which is known for its beautiful views of the Eiffel Tower, is located in the 7th arrondissement, has high ratings from travelers, and generally falls within the budget depending on the season. It embodies the ideal combination of location, quality, and affordability that aligns with the user's preferences, enhancing their overall travel experience in Paris.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Mock data for the travel recommendation\n",
    "user_preferences = {\n",
    "    \"location\": \"Paris\",\n",
    "    \"budget\": 200,\n",
    "    \"preferences\": [\"proximity to attractions\", \"user ratings\"],\n",
    "}\n",
    "\n",
    "# Input reasoning factors for the GPT model\n",
    "reasoning_prompt = f\"\"\"\n",
    "You are an AI-powered travel assistant. Explain your reasoning behind a hotel recommendation for a user traveling to {user_preferences['location']}.\n",
    "Consider:\n",
    "1. Proximity to popular attractions.\n",
    "2. High ratings from similar travelers.\n",
    "3. Competitive pricing within ${user_preferences['budget']} budget.\n",
    "4. Preferences: {user_preferences['preferences']}.\n",
    "Provide a clear, transparent self-explanation.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a reflective travel assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": reasoning_prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print self-explanation\n",
    "print(\"Agent Self-Explanation:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8b23e",
   "metadata": {},
   "source": [
    "#### Using Crew AI\n",
    "\n",
    "Our previous example was pretty simple and didn't use Agents. But with an agentic system you may have agents actually lookup hotels appropriate to the user query using tools. Subsequently, a second agent may perform the self-explanation transparency on the response. Let's first define a tool that will respond back with mock hotel data based on price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8632a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "\n",
    "# Tool 1\n",
    "@tool(\"Recommend hotels based on user query.\")\n",
    "def recommend_hotel(cost_per_night: int) -> str:\n",
    "    \"\"\"\n",
    "    Returns hotels based on cost per night.\n",
    "\n",
    "    Args:\n",
    "        cost_per_night (int): User's preference of hotel room per night cost. \n",
    "            \n",
    "    \"\"\"\n",
    "    static_hotels = [\n",
    "        {\n",
    "            \"hotel_name\": \"Le Royal Monceau Raffles\",\n",
    "            \"price_per_night\": 1200,\n",
    "            \"transportation_convenience\": \"convenient\",\n",
    "            \"location\": \"8th arrondissement\",\n",
    "            \"nearest_metro\": \"Charles de Gaulle-Ã‰toile\",\n",
    "            \"distance_from_metro\": '1 km'\n",
    "        },\n",
    "        {\n",
    "            \"hotel_name\": \"Citadines Les Halles\",\n",
    "            \"price_per_night\": 250,\n",
    "            \"transportation_convenience\": \"convenient\",\n",
    "            \"location\": \"1st arrondissement\",\n",
    "            \"nearest_metro\": \"Les Halles\",\n",
    "            \"distance_from_metro\": '2.8 km'\n",
    "        },\n",
    "        {\n",
    "            \"hotel_name\": \"Ibis Paris Montmartre\",\n",
    "            \"price_per_night\": 120,\n",
    "            \"transportation_convenience\": \"moderate\",\n",
    "            \"location\": \"18th arrondissement\",\n",
    "            \"nearest_metro\": \"Place de Clichy\",\n",
    "            \"distance_from_metro\": '5 km'\n",
    "        },\n",
    "        {\n",
    "            \"hotel_name\": \"Four Seasons Hotel George V\",\n",
    "            \"price_per_night\": 1500,\n",
    "            \"transportation_convenience\": \"convenient\",\n",
    "            \"location\": \"8th arrondissement\",\n",
    "            \"nearest_metro\": \"George V\",\n",
    "            \"distance_from_metro\": '1 km'\n",
    "        },\n",
    "        {\n",
    "            \"hotel_name\": \"Hotel du Petit Moulin\",\n",
    "            \"price_per_night\": 300,\n",
    "            \"transportation_convenience\": \"moderate\",\n",
    "            \"location\": \"3rd arrondissement\",\n",
    "            \"nearest_metro\": \"Saint-SÃ©bastien Froissart\",\n",
    "            \"distance_from_metro\": '1.9 km'\n",
    "        }\n",
    "    ]\n",
    "    matching_hotels = [\n",
    "        hotel for hotel in static_hotels \n",
    "        if cost_per_night <= hotel[\"price_per_night\"]\n",
    "    ]\n",
    "    return matching_hotels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66a8c9",
   "metadata": {},
   "source": [
    "Now we will perform the same transparency reasoning with a CrewAI Agent/Task combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bc2c462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotel Recommendation and Explanation:\n",
      "Hotel: Hotel du Petit Moulin\n",
      "\n",
      "Price/night: $300\n",
      "\n",
      "Reason: The only hotel within the specified budget of $300 a night and located in Paris is the Hotel du Petit Moulin. This hotel is situated in the 3rd arrondissement, which is known for its vibrant atmosphere and is relatively close to various attractions and dining options. It has a moderate transportation convenience with the nearest metro station being Saint-SÃ©bastien Froissart, approximately 1.9 km away. This hotel meets the userâ€™s requirement of affordability while providing a decent location to explore the city.\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai.process import Process\n",
    "\n",
    "travel_agent = Agent(\n",
    "    role=\"Travel Advisor\",\n",
    "    goal=\"Provide hotel recommendations with transparent reasoning.\",\n",
    "    backstory=\"\"\"\n",
    "    An AI travel advisor specializing in personalized travel planning. \n",
    "    You always explain the steps you take to arrive at a conclusion\n",
    "    \"\"\",\n",
    "    allow_delegation=False,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[recommend_hotel]\n",
    ")\n",
    "\n",
    "recommendation_task = Task(\n",
    "    name=\"Recommend hotel\",\n",
    "    description=\"\"\"\n",
    "    Recommend a hotel based on the user's query: \n",
    "    '{query}'.\n",
    "    \"\"\",\n",
    "    agent=travel_agent,\n",
    "    expected_output=\"The hotel recommendation and reasoning in the following format\\n\\nHotel: [Your answer]\\n\\nPrice/night: [The price]\\n\\nReason: [Detailed breakdown of your thought process]\"\n",
    ")\n",
    "\n",
    "travel_crew = Crew(\n",
    "    agents=[travel_agent],\n",
    "    tasks=[recommendation_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "travel_crew.kickoff(inputs={'query': \"I am looking for a hotel in Paris under $300 a night.\"})\n",
    "\n",
    "# Retrieve and print the output\n",
    "output = recommendation_task.output\n",
    "print(\"Hotel Recommendation and Explanation:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1c4b0",
   "metadata": {},
   "source": [
    "Not only does our Agent can lookup hotels using the tool but it clearly explains why it gave the recommendation as `Reason`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd239c6",
   "metadata": {},
   "source": [
    "### 2.2 Learning and Refinement: Using Self-Explanation to Identify Gaps\n",
    "\n",
    "While our self-explaining agent is great, the user may still not like the recommendation it gave. In which case the user may express their dissatisfaction with the recommendation. This is where we need learning and refinement of the approach. In our case we may extend the previous agent based system to now also include a second agent that can take user feedback and re-strategize on its approach to recommend a hotel. Note that in this case, sequential execution or parallel execution of the agents may not be appropriate, thus we need a hierarchical approach where a top level agent can manage the two agents and then delegate tasks accordingly.\n",
    "\n",
    "Lets define our learning and refinement agent that can take user feedback and it's previous recommendation in context and then complete the task by refining it's strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc669bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel Manager\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "    Recommend a hotel based on the user's query: \n",
      "    'I am looking for a hotel in Paris under $300 a night.'.\n",
      "    \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel Manager\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mTo proceed, I need to use the tool to find a hotel in Paris that fits the user's budget of under $300 per night.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mRecommend hotels based on user query.\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"cost_per_night\\\": 300}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "[{'hotel_name': 'Le Royal Monceau Raffles', 'price_per_night': 1200, 'transportation_convenience': 'convenient', 'location': '8th arrondissement', 'nearest_metro': 'Charles de Gaulle-Ã‰toile', 'distance_from_metro': '1 km'}, {'hotel_name': 'Four Seasons Hotel George V', 'price_per_night': 1500, 'transportation_convenience': 'convenient', 'location': '8th arrondissement', 'nearest_metro': 'George V', 'distance_from_metro': '1 km'}, {'hotel_name': 'Hotel du Petit Moulin', 'price_per_night': 300, 'transportation_convenience': 'moderate', 'location': '3rd arrondissement', 'nearest_metro': 'Saint-SÃ©bastien Froissart', 'distance_from_metro': '1.9 km'}]\n",
      "\n",
      "\n",
      "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
      "\n",
      "Tool Name: Recommend hotels based on user query.\n",
      "Tool Arguments: {'cost_per_night': {'description': None, 'type': 'int'}}\n",
      "Tool Description: \n",
      "    Returns hotels based on cost per night.\n",
      "\n",
      "    Args:\n",
      "        cost_per_night (int): User's preference of hotel room per night cost. \n",
      "            \n",
      "    \n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, only one name of [Recommend hotels based on user query.], just the name, exactly as it's written.\n",
      "Action Input: the input to the action, just a simple python dictionary, enclosed in curly braces, using \" to wrap keys and values.\n",
      "Observation: the result of the action\n",
      "\n",
      "Once all necessary information is gathered:\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel Manager\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Hotel: Hotel du Petit Moulin\n",
      "\n",
      "Price/night: $300\n",
      "\n",
      "Reason: After using the hotel recommendation tool with a user's budget of $300 per night, Hotel du Petit Moulin was found as a suitable match within the specified price range, as it costs exactly $300 per night. While it's not listed as the cheapest, it meets the user's maximum budget requirement and provides a balance between cost and location convenience in Paris's 3rd arrondissement.\u001b[00m\n",
      "\n",
      "\n",
      "Hotel: Hotel du Petit Moulin\n",
      "\n",
      "Price/night: $300\n",
      "\n",
      "Reason: After using the hotel recommendation tool with a user's budget of $300 per night, Hotel du Petit Moulin was found as a suitable match within the specified price range, as it costs exactly $300 per night. While it's not listed as the cheapest, it meets the user's maximum budget requirement and provides a balance between cost and location convenience in Paris's 3rd arrondissement.\n",
      "Upon reflecting on the previous recommendation of Hotel du Petit Moulin, I recognize an important oversight in my reasoning process. While the hotel met the price requirement of $300 per night and was located in a convenient district, I failed to consider the proximity to public transportation, which is a crucial factor for the user's travel needs. The preference for locations closer to metro stations was overlooked, leading to a recommendation that did not fully align with the user's preferences.\n",
      "\n",
      "To refine my recommendation process moving forward, I will incorporate the following key aspects:\n",
      "\n",
      "1. **User Preferences Analysis**: Always start with a thorough analysis of user preferences, placing special emphasis on transportation needs, including proximity to public transportation options such as metro stations and bus stops.\n",
      "\n",
      "2. **Location Evaluation**: Include a more detailed evaluation of the hotel's location in relation to various points of interest and public transport hubs. This will involve looking at the walking distance to the nearest metro station and the accessibility of major attractions.\n",
      "\n",
      "3. **Balancing Cost and Convenience**: While maintaining the focus on the budget, ensure that the balance between price and convenient access to transport options is emphasized in the final recommendations.\n",
      "\n",
      "4. **Feedback Integration**: Actively incorporate feedback from users in past recommendations into future decision-making, ensuring that each suggestion is tailored to better suit the user's expressed needs and preferences.\n",
      "\n",
      "5. **Tools and Resources Examination**: Utilize updated hotel recommendation tools that allow for better filtering based on public transport access, ensuring that the chosen accommodations also feature transportation accessibility as part of their advantages.\n",
      "\n",
      "With these refinements, I aim to provide recommendations that not only satisfy the budget requirement but also ensure that they meet the user's needs for convenient access to public transport, creating a more enjoyable and hassle-free travel experience. \n",
      "\n",
      "I will now proceed to find a suitable hotel recommendation that adheres to these updated criteria.\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai.process import Process\n",
    "\n",
    "reflective_travel_agent = Agent(\n",
    "    role=\"Self-Improving Travel Advisor\",\n",
    "    goal=\"Refine hotel recommendations based on user feedback to your previous recommendation to improve decision-making.\",\n",
    "    backstory=\"\"\"\n",
    "    A reflective AI travel advisor specializing in personalized travel planning that learns from user feedback. \n",
    "    When a user highlights an issue with a recommendation, it revisits its reasoning,\n",
    "    identifies overlooked factors, and updates its decision process accordingly.\n",
    "    \"\"\",\n",
    "    allow_delegation=False,\n",
    "    llm='gpt-4o-mini',\n",
    "    #tools=[recommend_hotel] # <-- This agent also uses the same tool to refine it's recommendation\n",
    ")\n",
    "\n",
    "feedback_task = Task(\n",
    "    description=\"\"\"\n",
    "    Based on your previous recommendation:\n",
    "    '{recommendation}'\n",
    "\n",
    "    Reflect on the user's feedback to the hotel recommendation:\n",
    "    '{query}'\n",
    "\n",
    "    - Identify any oversight in your previous reasoning process.\n",
    "    - Update your reasoning process to include aspects that were missed.\n",
    "    - Provide the refined steps that you will use to recommend hotels.\n",
    "    \"\"\",\n",
    "    expected_output=\"\"\"\n",
    "    A refined explanation that acknowledges the oversight, includes missed factors,\n",
    "    and provides a revised steps to recommend hotels tailored to the user's feedback.\n",
    "    \"\"\",\n",
    "    agent=reflective_travel_agent,\n",
    "    context=[recommendation_task] \n",
    ")\n",
    "\n",
    "\n",
    "travel_feedback_crew = Crew(\n",
    "    agents=[reflective_travel_agent],\n",
    "    tasks=[feedback_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# We will run the travel_crew from the previous example with user's query\n",
    "response1 = travel_crew.kickoff(inputs={'query': \"I am looking for a hotel in Paris under $300 a night.\"})\n",
    "print(response1)\n",
    "\n",
    "\n",
    "response2 = travel_feedback_crew.kickoff(inputs={'recommendation': response1,\n",
    "                                                 'query': \"The hotel you recommended was too far from public transport. I prefer locations closer to metro stations.\"})\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa021faf",
   "metadata": {},
   "source": [
    "### 2.3. User Engagement and Collaboration: Enabling Interactive Explanations\n",
    "\n",
    "In this example, the agent provides explanations for its decisions and engages users to refine suggestions interactively. Just like before, we can have an Agent/Task pair with CrewAI framework whose job is to interact with the users by asking clarifying questions about their preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58e4bc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Interactive Dialogue with User...\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mCollaborative AI Travel Assistant\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "    Facilitate an interactive dialogue with the user.\n",
      "\n",
      "    - Here is the initial recommendation: \"I recommend Hotel LumiÃ¨re in Paris for its proximity to the Eiffel Tower, high ratings, and budget-friendly price.\"\n",
      "    - The user has asked: \"Why did you prioritize proximity to attractions over public transport access?\"\n",
      "\n",
      "    Respond by:\n",
      "    1. Explaining the reasoning behind prioritizing proximity to attractions.\n",
      "    2. Inviting the user to clarify whether proximity to public transport is more important.\n",
      "    \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mCollaborative AI Travel Assistant\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Thank you for your insightful question! I prioritized proximity to attractions, like the Eiffel Tower, because many travelers to Paris often want to maximize their sightseeing experience. Being close to major landmarks can save time and allow for a more leisurely visit, which enhances the overall travel experience. \n",
      "\n",
      "However, I completely understand the importance of public transport access, especially in a city like Paris where it can significantly affect your mobility and convenience during your stay. Public transportation can help you explore further and reach less touristy areas, which can be just as enjoyable.\n",
      "\n",
      "Could you share more about what matters most to you? Would you prefer to stay closer to public transport, or is the allure of being near the main attractions the priority for your trip? Your preferences will help me make an even more tailored recommendation!\u001b[00m\n",
      "\n",
      "\n",
      "Final Interactive Response:\n",
      "Thank you for your insightful question! I prioritized proximity to attractions, like the Eiffel Tower, because many travelers to Paris often want to maximize their sightseeing experience. Being close to major landmarks can save time and allow for a more leisurely visit, which enhances the overall travel experience. \n",
      "\n",
      "However, I completely understand the importance of public transport access, especially in a city like Paris where it can significantly affect your mobility and convenience during your stay. Public transportation can help you explore further and reach less touristy areas, which can be just as enjoyable.\n",
      "\n",
      "Could you share more about what matters most to you? Would you prefer to stay closer to public transport, or is the allure of being near the main attractions the priority for your trip? Your preferences will help me make an even more tailored recommendation!\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai.process import Process\n",
    "\n",
    "# Step 1: Define the Collaborative Agent\n",
    "collaborative_travel_agent = Agent(\n",
    "    role=\"Collaborative AI Travel Assistant\",\n",
    "    goal=\"\"\"\n",
    "    Engage in an interactive dialogue with the user to clarify hotel recommendations.\n",
    "    Explain reasoning for prioritizing certain factors and invite the user to share their preferences.\n",
    "    \"\"\",\n",
    "    backstory=\"\"\"\n",
    "    An AI travel assistant that values user input and ensures recommendations are well-aligned with user needs.\n",
    "    It provides clear explanations for its decisions and encourages collaborative planning.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "interactive_task = Task(\n",
    "    description=\"\"\"\n",
    "    Facilitate an interactive dialogue with the user.\n",
    "\n",
    "    - Here is the initial recommendation: {initial_recommendation}\n",
    "    - The user has asked: {user_query}\n",
    "\n",
    "    Respond by:\n",
    "    1. Explaining the reasoning behind prioritizing proximity to attractions.\n",
    "    2. Inviting the user to clarify whether proximity to public transport is more important.\n",
    "    \"\"\",\n",
    "    expected_output=\"\"\"\n",
    "    A clear and polite response explaining the reasoning and inviting the user to share further input.\n",
    "    \"\"\",\n",
    "    agent=collaborative_travel_agent\n",
    ")\n",
    "\n",
    "# Step 3: Assemble the Crew\n",
    "interactive_crew = Crew(\n",
    "    agents=[collaborative_travel_agent],\n",
    "    tasks=[interactive_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Define the Task for Clarification Dialogue\n",
    "\n",
    "# Initial recommendation \n",
    "initial_recommendation = \"I recommend Hotel LumiÃ¨re in Paris for its proximity to the Eiffel Tower, high ratings, and budget-friendly price.\"\n",
    "user_query = \"Why did you prioritize proximity to attractions over public transport access?\"\n",
    "\n",
    "# Step 4: Run the Crew and Output the Results\n",
    "print(\"Starting Interactive Dialogue with User...\\n\")\n",
    "result = interactive_crew.kickoff(inputs={\"initial_recommendation\": initial_recommendation, \"user_query\": user_query})\n",
    "\n",
    "print(\"Final Interactive Response:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f01747",
   "metadata": {},
   "source": [
    "# 3. Self Modeling - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e14cd4",
   "metadata": {},
   "source": [
    "The `ReflectiveTravelAgentWithSelfModeling` class represents a sophisticated travel recommendation system that utilizes **self-modeling** to enhance its decision-making and adaptability. \n",
    "\n",
    "### 1. **Initialization:**\n",
    "   - **Self-Model and Knowledge Base:** The agent starts with an internal model that includes its goals and a knowledge base. \n",
    "     - **Goals:** Initially, the goals are set to provide personalized recommendations, optimize user satisfaction, and not prioritize eco-friendly options by default.\n",
    "     - **Knowledge Base:** It contains information about various travel destinations, including their ratings, costs, luxury levels, and sustainability. This base also tracks user preferences.\n",
    "\n",
    "### 2. **Updating Goals:**\n",
    "   - **Adapting to Preferences:** When new user preferences are provided, the agent can update its goals accordingly. For example, if the user prefers eco-friendly options, the agent will adjust its goals to prioritize recommending sustainable travel options. Similarly, if the userâ€™s budget changes, the agent will refocus on cost-effective recommendations.\n",
    "\n",
    "### 3. **Updating Knowledge Base:**\n",
    "   - **Incorporating Feedback:** After receiving feedback from users, the agent updates its knowledge base. If the feedback is positive, the agent increases the rating of the recommended destination. If the feedback is negative, the rating is decreased. This helps the agent refine its recommendations based on real user experiences.\n",
    "\n",
    "### 4. **Making Recommendations:**\n",
    "   - **Calculating Scores:** The agent evaluates each destination by calculating a score based on its rating and, if eco-friendly options are a goal, it adjusts the score by adding the sustainability rating.\n",
    "   - **Selecting the Best Destination:** The destination with the highest score is recommended to the user. This process ensures that the recommendation aligns with both user preferences and the agentâ€™s goals.\n",
    "\n",
    "### 5. **Engaging with the User:**\n",
    "   - **Providing Recommendations:** The agent presents the recommended destination to the user and asks for feedback.\n",
    "   - **Feedback Handling:** The feedback (positive or negative) is used to update the knowledge base, which helps improve future recommendations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveTravelAgentWithSelfModeling:\n",
    "    def __init__(self):\n",
    "        # Initialize the agent with a self-model that includes goals and a knowledge base\n",
    "        self.self_model = {\n",
    "            \"goals\": {\n",
    "                \"personalized_recommendations\": True,\n",
    "                \"optimize_user_satisfaction\": True,\n",
    "                \"eco_friendly_options\": False  # Default: Not prioritizing eco-friendly options\n",
    "            },\n",
    "            \"knowledge_base\": {\n",
    "                \"destinations\": {\n",
    "                    \"Paris\": {\"rating\": 4.8, \"cost\": 2000, \"luxury\": 0.9, \"sustainability\": 0.3},\n",
    "                    \"Bangkok\": {\"rating\": 4.5, \"cost\": 1500, \"luxury\": 0.7, \"sustainability\": 0.6},\n",
    "                    \"Barcelona\": {\"rating\": 4.7, \"cost\": 1800, \"luxury\": 0.8, \"sustainability\": 0.7}\n",
    "                },\n",
    "                \"user_preferences\": {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def update_goals(self, new_preferences):\n",
    "        \"\"\"Update the agent's goals based on new user preferences.\"\"\"\n",
    "        if new_preferences.get(\"eco_friendly\"):\n",
    "            self.self_model[\"goals\"][\"eco_friendly_options\"] = True\n",
    "            print(\"Updated goal: Prioritize eco-friendly travel options.\")\n",
    "        if new_preferences.get(\"adjust_budget\"):\n",
    "            print(\"Updated goal: Adjust travel options based on new budget constraints.\")\n",
    "    \n",
    "    def update_knowledge_base(self, feedback):\n",
    "        \"\"\"Update the agent's knowledge base based on user feedback.\"\"\"\n",
    "        destination = feedback[\"destination\"]\n",
    "        if feedback[\"positive\"]:\n",
    "            # Increase rating for positive feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] += 0.1\n",
    "            print(f\"Positive feedback received for {destination}; rating increased.\")\n",
    "        else:\n",
    "            # Decrease rating for negative feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] -= 0.2\n",
    "            print(f\"Negative feedback received for {destination}; rating decreased.\")\n",
    "    \n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"Recommend a destination based on user preferences and the agent's self-model.\"\"\"\n",
    "        # Store user preferences in the agent's self-model\n",
    "        self.self_model[\"knowledge_base\"][\"user_preferences\"] = user_preferences\n",
    "        \n",
    "        # Update agent's goals based on new preferences\n",
    "        if user_preferences.get(\"eco_friendly\"):\n",
    "            self.update_goals(user_preferences)\n",
    "        \n",
    "        # Calculate scores for each destination\n",
    "        best_destination = None\n",
    "        highest_score = 0\n",
    "        for destination, info in self.self_model[\"knowledge_base\"][\"destinations\"].items():\n",
    "            score = info[\"rating\"]\n",
    "            if self.self_model[\"goals\"][\"eco_friendly_options\"]:\n",
    "                # Boost score for eco-friendly options if that goal is prioritized\n",
    "                score += info[\"sustainability\"]\n",
    "            \n",
    "            # Update the best destination if current score is higher\n",
    "            if score > highest_score:\n",
    "                best_destination = destination\n",
    "                highest_score = score\n",
    "        \n",
    "        return best_destination\n",
    "\n",
    "    def engage_with_user(self, destination):\n",
    "        \"\"\"Simulate user engagement by providing the recommendation and receiving feedback.\"\"\"\n",
    "        print(f\"Recommended destination: {destination}\")\n",
    "        # Simulate receiving user feedback (e.g., through input in a real application)\n",
    "        feedback = input(f\"Did you like the recommendation of {destination}? (yes/no): \").strip().lower()\n",
    "        positive_feedback = feedback == \"yes\"\n",
    "        return {\"destination\": destination, \"positive\": positive_feedback}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6fdea",
   "metadata": {},
   "source": [
    "The provided code snippet is designed to simulate the usage of the `ReflectiveTravelAgentWithSelfModeling` class. \n",
    "\n",
    "### 1. **Creating an Instance of the Agent:**\n",
    "   ```python\n",
    "   agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "   ```\n",
    "   - **Purpose:** Initializes a new instance of the `ReflectiveTravelAgentWithSelfModeling` class.\n",
    "   - **Outcome:** This instance represents a travel agent equipped with self-modeling capabilities, including goal management and a knowledge base.\n",
    "\n",
    "### 2. **Setting User Preferences:**\n",
    "   ```python\n",
    "   user_preferences = {\n",
    "       \"budget\": 0.6,            # Moderate budget constraint\n",
    "       \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "       \"adventure\": 0.7,         # High preference for adventure\n",
    "       \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "   }\n",
    "   ```\n",
    "   - **Purpose:** Defines a set of preferences provided by the user.\n",
    "   - **Outcome:** These preferences indicate that the user has a moderate budget, moderate luxury preferences, a high interest in adventure, and a strong preference for eco-friendly options.\n",
    "\n",
    "### 3. **Getting a Recommendation:**\n",
    "   ```python\n",
    "   recommendation = agent.recommend_destination(user_preferences)\n",
    "   ```\n",
    "   - **Purpose:** Requests a travel destination recommendation from the agent based on the provided user preferences.\n",
    "   - **Outcome:** The agent processes the preferences, updates its goals if necessary (e.g., prioritizing eco-friendly options), and selects the best destination to recommend.\n",
    "\n",
    "### 4. **Engaging with the User:**\n",
    "   ```python\n",
    "   feedback = agent.engage_with_user(recommendation)\n",
    "   ```\n",
    "   - **Purpose:** Simulates interaction with the user by presenting the recommendation and gathering feedback.\n",
    "   - **Outcome:** The user provides feedback on the recommended destination, which is used to evaluate the effectiveness of the recommendation.\n",
    "\n",
    "### 5. **Updating the Knowledge Base:**\n",
    "   ```python\n",
    "   agent.update_knowledge_base(feedback)\n",
    "   ```\n",
    "   - **Purpose:** Updates the agentâ€™s knowledge base with the feedback received from the user.\n",
    "   - **Outcome:** The agent adjusts its knowledge base by modifying ratings or other attributes based on whether the feedback was positive or negative. This update helps improve future recommendations by refining the agent's understanding of user preferences and destination qualities.\n",
    "\n",
    "### Summary:\n",
    "In essence, this code snippet demonstrates how the `ReflectiveTravelAgentWithSelfModeling` class operates in a simulated environment. It initializes the agent, sets user preferences, obtains a recommendation, engages the user for feedback, and updates the agentâ€™s knowledge base based on that feedback. This simulation helps illustrate the agentâ€™s self-modeling capabilities and its ability to adapt and improve recommendations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated goal: Prioritize eco-friendly travel options.\n",
      "Recommended destination: Barcelona\n",
      "Did you like the recommendation of Barcelona? (yes/no): no\n",
      "Negative feedback received for Barcelona; rating decreased.\n"
     ]
    }
   ],
   "source": [
    "# Simulating agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the reflective travel agent with self-modeling\n",
    "    agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "    \n",
    "    # Example user preferences including a focus on eco-friendly options\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.6,            # Moderate budget constraint\n",
    "        \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "        \"adventure\": 0.7,         # High preference for adventure\n",
    "        \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "    }\n",
    "    \n",
    "    # Get the recommended destination based on user preferences\n",
    "    recommendation = agent.recommend_destination(user_preferences)\n",
    "    \n",
    "    # Engage with the user to provide feedback on the recommendation\n",
    "    feedback = agent.engage_with_user(recommendation)\n",
    "    \n",
    "    # Update the knowledge base with the user feedback\n",
    "    agent.update_knowledge_base(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3c2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
